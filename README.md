# Papers-Continual-RL
I am constantly collecting papers on Continual Reinforcement Learning published on high-profile ML conferences or journals. Although the theoretical foundation of continual RL is not clear, it is still a valuable research field as it helps to address the challenges in the **non-stationary environments** for RL algorithms. Please feel free to let us know if you feel we have missed some important papers.

**Contact** : [Ke Sun](https://sites.google.com/view/kesun), ksun6@ualberta.ca


## 2024

* [CPPO: Continual Learning for Reinforcement Learning with Human Feedback](https://openreview.net/forum?id=86zAUE80pP) (ICLR 2024)
> This paper is the first to consider the continual RLHF with dynamic preference learning, by using the sample-wise reweighting method based on the performance and variance.


## 2023

* [Building a Subspace of Policies for Scalable Continual Learning](https://arxiv.org/abs/2211.10445) (ICLR 2023)
> This paper incrementally expands the subspace of policies to balance the agent's size and performance in continual RL. Although the result is very reasonable, it may suffer from the high computation, and increasing the agent's size may not be admissible in practice.

* [A Definition of Continual Reinforcement Learning](https://arxiv.org/abs/2307.11046) (NeurIPS 2023)
> The authors are the first to provide a conceptual definition of continual RL, but unfortunately, no practical algorithms are proposed.


## 2022




## 2021

* [Continual World: A Robotic Benchmark For Continual Reinforcement Learning](https://arxiv.org/abs/2105.10919) (NeurIPS 2021)
> Benchmark: Continual World, 10 manipulation tasks from MetaWorld

## 2020 and Before

* [Continual Reinforcement Learning with Complex Synapses](https://arxiv.org/abs/1802.07239) (ICML 2018)
> This paper incorporates a synaptic model in RL agents to mitigate catastrophic forgetting in continual RL. This study is inspired by neuroscience, but its experiments are restricted on tabular experiments.

